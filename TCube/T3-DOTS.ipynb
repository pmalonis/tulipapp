{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import math\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation\n",
    "import utils\n",
    "import data2graph\n",
    "from finetuned import T5FineTuner, BARTFineTuner, generate, generate_beam, graph2text_nobeam, graph2text_nobeam_ngram_es, graph2text_nobeam_topk, graph2text_nobeam_topp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "import language_tool_python\n",
    "from lexical_diversity import lex_div as ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "def grammar_score(input_text):\n",
    "    errors = len(tool.check(input_text))\n",
    "    clean_text = input_text.translate(str.maketrans('', '', string.punctuation))\n",
    "    clean_text = list(filter(None, clean_text.split(' ')))\n",
    "    num_words = len(clean_text)\n",
    "    return float(1-(errors/num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Fine-Tuned PLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BARTFineTuner(\n",
       "  (model): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): Embedding(50268, 1024)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): Embedding(50268, 1024)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): Embedding(50268, 1024)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "# cuda1 = torch.device(\"cuda:1\")\n",
    "#cuda3 = torch.device(\"cuda:3\")\n",
    "\n",
    "t5 = T5FineTuner.load_from_checkpoint(\"T5Models/T5Both.ckpt\")\n",
    "bart = BARTFineTuner.load_from_checkpoint(\"BARTModels/BARTBoth.ckpt\")\n",
    "\n",
    "t5.to(cuda0)\n",
    "bart.to(cuda0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import DOTS Dataset\n",
    "ds_dots = pd.read_csv(\"Data/DOTS/Exports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Country:  United States\n",
      "Waves Detected:  United States\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  United States\n",
      "Simple Generation Complete:  United States\n",
      "Top-k Complete:  United States\n",
      "Top-p Complete:  United States\n",
      "RE Scores Computed:  United States\n",
      "TTE Scores Computed:  United States\n",
      "Grammar Scores Computed:  United States\n",
      "Processing Country:  India\n",
      "Waves Detected:  India\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  India\n",
      "Simple Generation Complete:  India\n",
      "Top-k Complete:  India\n",
      "Top-p Complete:  India\n",
      "RE Scores Computed:  India\n",
      "TTE Scores Computed:  India\n",
      "Grammar Scores Computed:  India\n",
      "Processing Country:  Brazil\n",
      "Waves Detected:  Brazil\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  Brazil\n",
      "Simple Generation Complete:  Brazil\n",
      "Top-k Complete:  Brazil\n",
      "Top-p Complete:  Brazil\n",
      "RE Scores Computed:  Brazil\n",
      "TTE Scores Computed:  Brazil\n",
      "Grammar Scores Computed:  Brazil\n",
      "Processing Country:  USSR\n",
      "Waves Detected:  USSR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  USSR\n",
      "Simple Generation Complete:  USSR\n",
      "Top-k Complete:  USSR\n",
      "Top-p Complete:  USSR\n",
      "RE Scores Computed:  USSR\n",
      "TTE Scores Computed:  USSR\n",
      "Grammar Scores Computed:  USSR\n",
      "Processing Country:  United Kingdom\n",
      "Waves Detected:  United Kingdom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  United Kingdom\n",
      "Simple Generation Complete:  United Kingdom\n",
      "Top-k Complete:  United Kingdom\n",
      "Top-p Complete:  United Kingdom\n",
      "RE Scores Computed:  United Kingdom\n",
      "TTE Scores Computed:  United Kingdom\n",
      "Grammar Scores Computed:  United Kingdom\n",
      "Processing Country:  France\n",
      "Waves Detected:  France\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  France\n",
      "Simple Generation Complete:  France\n",
      "Top-k Complete:  France\n",
      "Top-p Complete:  France\n",
      "RE Scores Computed:  France\n",
      "TTE Scores Computed:  France\n",
      "Grammar Scores Computed:  France\n",
      "Processing Country:  Spain\n",
      "Waves Detected:  Spain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  Spain\n",
      "Simple Generation Complete:  Spain\n",
      "Top-k Complete:  Spain\n",
      "Top-p Complete:  Spain\n",
      "RE Scores Computed:  Spain\n",
      "TTE Scores Computed:  Spain\n",
      "Grammar Scores Computed:  Spain\n",
      "Processing Country:  Italy\n",
      "Waves Detected:  Italy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  Italy\n",
      "Simple Generation Complete:  Italy\n",
      "Top-k Complete:  Italy\n",
      "Top-p Complete:  Italy\n",
      "RE Scores Computed:  Italy\n",
      "TTE Scores Computed:  Italy\n",
      "Grammar Scores Computed:  Italy\n",
      "Processing Country:  Turkey\n",
      "Waves Detected:  Turkey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  Turkey\n",
      "Simple Generation Complete:  Turkey\n",
      "Top-k Complete:  Turkey\n",
      "Top-p Complete:  Turkey\n",
      "RE Scores Computed:  Turkey\n",
      "TTE Scores Computed:  Turkey\n",
      "Grammar Scores Computed:  Turkey\n",
      "Processing Country:  Germany\n",
      "Waves Detected:  Germany\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  Germany\n",
      "Simple Generation Complete:  Germany\n",
      "Top-k Complete:  Germany\n",
      "Top-p Complete:  Germany\n",
      "RE Scores Computed:  Germany\n",
      "TTE Scores Computed:  Germany\n",
      "Grammar Scores Computed:  Germany\n"
     ]
    }
   ],
   "source": [
    "countries = ['United States', 'India', 'Brazil', 'USSR', 'United Kingdom', 'France', 'Spain', 'Italy' , 'Turkey', 'Germany']\n",
    "\n",
    "#RE Scores\n",
    "template_re_scores = []\n",
    "t5_re_scores = []\n",
    "t5_re_scores_topk = []\n",
    "t5_re_scores_topp = []\n",
    "bart_re_scores = []\n",
    "bart_re_scores_topk = []\n",
    "bart_re_scores_topp = []\n",
    "\n",
    "#Diveristy Scores\n",
    "template_tte_scores = []\n",
    "t5_tte_scores = []\n",
    "t5_tte_scores_topk = []\n",
    "t5_tte_scores_topp = []\n",
    "bart_tte_scores = []\n",
    "bart_tte_scores_topk = []\n",
    "bart_tte_scores_topp = []\n",
    "\n",
    "#Grammar Scores\n",
    "t5_g_scores = []\n",
    "t5_g_scores_topk = []\n",
    "t5_g_scores_topp = []\n",
    "bart_g_scores = []\n",
    "bart_g_scores_topk = []\n",
    "bart_g_scores_topp = []\n",
    "\n",
    "#Grammar Mistakes\n",
    "t5_g_mistake = []\n",
    "t5_g_mistake_topk = []\n",
    "t5_g_mistake_topp = []\n",
    "bart_g_mistake = []\n",
    "bart_g_mistake_topk = []\n",
    "bart_g_mistake_topp = []\n",
    "\n",
    "for iso in countries:\n",
    "    \n",
    "    #Load and Process Data\n",
    "    country = ds_dots.loc[ds_dots['Location'] == iso]\n",
    "    time = country.columns.tolist()[1:]\n",
    "    for row in country.iterrows():\n",
    "        values = row[1]\n",
    "    values = [str(x) for x in values]\n",
    "    values = [float(re.sub(',', '', x)) for x in values[1:]]\n",
    "    time = [re.sub('M', '-', x) for x in time]\n",
    "    country = pd.DataFrame(list(zip(time, values)), columns = ['Date', 'Exports'])\n",
    "    \n",
    "    country = country.fillna(0)\n",
    "    country.Date = pd.to_datetime(country.Date)\n",
    "    country['month'] = pd.DatetimeIndex(country['Date']).month\n",
    "    country['month'] = country['month'].apply(lambda x: calendar.month_name[x])\n",
    "    country['year'] = pd.DatetimeIndex(country['Date']).year\n",
    "    country.set_index(['Date'],inplace=True)\n",
    "    \n",
    "    country = country[['Exports','month', 'year']].reset_index().drop(columns=['Date'])\n",
    "    country_exports_raw = country['Exports'].tolist()\n",
    "\n",
    "    #Log-normalize data\n",
    "    trans = np.ma.log(country_exports_raw)\n",
    "    country_exports = trans.filled(0)\n",
    "    \n",
    "    print(\"Processing Country: \", iso)\n",
    "    \n",
    "    #Detecting Waves\n",
    "    embeds, cluster_labels = segmentation.tslr_rep(country_exports, k=3, tolerance=1e-4)\n",
    "    cluster_arrangement = utils.find_contiguous(cluster_labels)\n",
    "    indices = utils.find_indices(cluster_arrangement)\n",
    "    wave_indices = utils.find_waves(country_exports_raw, indices, tolerance=7)\n",
    "    \n",
    "    print(\"Waves Detected: \", iso)\n",
    "\n",
    "    #Detecting Trends\n",
    "    segmentation_results = segmentation.swab(country_exports, 0.1, 3, 3)\n",
    "    filtered_results = segmentation.re_segment(segmentation_results, country_exports)\n",
    "    trends = segmentation.find_trend(filtered_results, country_exports)\n",
    "    \n",
    "    print(\"Trends Detected: \", iso)\n",
    "    \n",
    "    location = iso\n",
    "    \n",
    "    graph, essentials = data2graph.build_graph_exports_form1(\"Merchandise Exports\", location, wave_indices, trends, country, country_exports_raw)\n",
    "    #Template Narrative\n",
    "    template_text = data2graph.build_template_exports_nums(\"Merchandise Exports\", location, wave_indices, trends, country, country_exports_raw)\n",
    "    t5_prefix = 'translate Graph to English: '\n",
    "    \n",
    "    #Simple PLM Generation\n",
    "    t5_narrative = graph2text_nobeam(t5, graph, t5_prefix, 512, cuda0)\n",
    "    bart_narrative = graph2text_nobeam(bart , graph, \"\", 512, cuda0)\n",
    "    bart_narrative = re.sub('</s>' , '', bart_narrative)\n",
    "    \n",
    "    print(\"Simple Generation Complete: \", iso)\n",
    "    \n",
    "    #Top-k at 50\n",
    "    t5_narrative_topk = graph2text_nobeam_topk(t5, graph, t5_prefix, 50, 512, cuda0)\n",
    "    bart_narrative_topk = graph2text_nobeam_topk(bart, graph, \"\", 50, 512, cuda0)\n",
    "    bart_narrative_topk = re.sub('</s>' , '', bart_narrative_topk)\n",
    "    \n",
    "    print(\"Top-k Complete: \", iso)\n",
    "    \n",
    "    #Top-p at 0.92\n",
    "    t5_narrative_topp = graph2text_nobeam_topp(t5, graph, t5_prefix, 0.92, 512, cuda0)\n",
    "    bart_narrative_topp = graph2text_nobeam_topp(bart, graph, \"\", 0.92, 512, cuda0)\n",
    "    bart_narrative_topp = re.sub('</s>' , '', bart_narrative_topp)\n",
    "    \n",
    "    print(\"Top-p Complete: \", iso)\n",
    "    \n",
    "    #RE Scores\n",
    "    template_re_scores.append(textstat.flesch_reading_ease(template_text))\n",
    "    t5_re_scores.append(textstat.flesch_reading_ease(t5_narrative))\n",
    "    t5_re_scores_topk.append(textstat.flesch_reading_ease(t5_narrative_topk))\n",
    "    t5_re_scores_topp.append(textstat.flesch_reading_ease(t5_narrative_topp))\n",
    "    bart_re_scores.append(textstat.flesch_reading_ease(bart_narrative))\n",
    "    bart_re_scores_topk.append(textstat.flesch_reading_ease(bart_narrative_topk))\n",
    "    bart_re_scores_topp.append(textstat.flesch_reading_ease(bart_narrative_topp))\n",
    "    \n",
    "    print(\"RE Scores Computed: \", iso)\n",
    "    \n",
    "    #Diveristy Scores\n",
    "    template_tte_scores.append(ld.ttr(ld.flemmatize(template_text)))\n",
    "    t5_tte_scores.append(ld.ttr(ld.flemmatize(t5_narrative)))\n",
    "    t5_tte_scores_topk.append(ld.ttr(ld.flemmatize(t5_narrative_topk)))\n",
    "    t5_tte_scores_topp.append(ld.ttr(ld.flemmatize(t5_narrative_topp)))\n",
    "    bart_tte_scores.append(ld.ttr(ld.flemmatize(bart_narrative)))\n",
    "    bart_tte_scores_topk.append(ld.ttr(ld.flemmatize(bart_narrative_topk)))\n",
    "    bart_tte_scores_topp.append(ld.ttr(ld.flemmatize(bart_narrative_topp)))\n",
    "    \n",
    "    print(\"TTE Scores Computed: \", iso)\n",
    "    \n",
    "    #Grammar Scores\n",
    "    gs = grammar_score(t5_narrative)\n",
    "    t5_g_scores.append(gs)\n",
    "    if gs != 1.0:\n",
    "        t5_g_mistake.append((graph, t5_narrative))\n",
    "    \n",
    "    gs = grammar_score(t5_narrative_topk)\n",
    "    t5_g_scores_topk.append(gs)\n",
    "    if gs != 1.0:\n",
    "        t5_g_mistake_topk.append((graph, t5_narrative_topk))\n",
    "    \n",
    "    gs = grammar_score(t5_narrative_topp)\n",
    "    t5_g_scores_topp.append(gs)\n",
    "    if gs != 1.0:\n",
    "        t5_g_mistake_topp.append((graph, t5_narrative_topp))\n",
    "    \n",
    "    gs = grammar_score(bart_narrative)                          \n",
    "    bart_g_scores.append(gs)\n",
    "    if gs != 1.0:\n",
    "        bart_g_mistake.append((graph, bart_narrative))\n",
    "        \n",
    "    gs = grammar_score(bart_narrative_topk)\n",
    "    bart_g_scores_topk.append(gs)\n",
    "    if gs != 1.0:\n",
    "        bart_g_mistake_topk.append((graph, bart_narrative_topk))\n",
    "    \n",
    "    gs = grammar_score(bart_narrative_topp)\n",
    "    bart_g_scores_topp.append(gs)\n",
    "    if gs != 1.0:\n",
    "        bart_g_mistake_topp.append((graph, bart_narrative_topp))\n",
    "    \n",
    "    print(\"Grammar Scores Computed: \", iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** RE Scores ***\n",
      "template_re_scores:  54.736000000000004\n",
      "t5_re_scores:  67.53600000000002\n",
      "t5_re_scores_topk:  67.57399999999998\n",
      "t5_re_scores_topp:  66.272\n",
      "bart_re_scores:  67.30699999999999\n",
      "bart_re_scores_topk:  69.46700000000001\n",
      "bart_re_scores_topp:  68.358\n",
      "\n",
      "\n",
      "*** Diversity Scores ***\n",
      "template_tte_scores:  0.4519841917389087\n",
      "t5_tte_scores:  0.47294858147614677\n",
      "t5_tte_scores_topk:  0.5071300125808091\n",
      "t5_tte_scores_topp:  0.4870217016673051\n",
      "bart_tte_scores:  0.46536980951459095\n",
      "bart_tte_scores_topk:  0.46927789161856526\n",
      "bart_tte_scores_topp:  0.47230417440905653\n",
      "\n",
      "\n",
      "*** Grammar Scores ***\n",
      "t5_g_scores:  0.9949909050606554\n",
      "t5_g_scores_topk:  0.9816560771206655\n",
      "t5_g_scores_topp:  0.9874332566242146\n",
      "bart_g_scores:  0.9743891308887622\n",
      "bart_g_scores_topk:  0.9660297270530052\n",
      "bart_g_scores_topp:  0.9701897078169734\n"
     ]
    }
   ],
   "source": [
    "#RE Scores\n",
    "print(\"*** RE Scores ***\")\n",
    "print(\"template_re_scores: \", np.mean(template_re_scores))\n",
    "print(\"t5_re_scores: \", np.mean(t5_re_scores))\n",
    "print(\"t5_re_scores_topk: \", np.mean(t5_re_scores_topk))\n",
    "print(\"t5_re_scores_topp: \", np.mean(t5_re_scores_topp))\n",
    "print(\"bart_re_scores: \", np.mean(bart_re_scores))\n",
    "print(\"bart_re_scores_topk: \", np.mean(bart_re_scores_topk))\n",
    "print(\"bart_re_scores_topp: \", np.mean(bart_re_scores_topp))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"*** Diversity Scores ***\")\n",
    "#Diveristy Scores\n",
    "print(\"template_tte_scores: \", np.mean(template_tte_scores))\n",
    "print(\"t5_tte_scores: \", np.mean(t5_tte_scores))\n",
    "print(\"t5_tte_scores_topk: \", np.mean(t5_tte_scores_topk))\n",
    "print(\"t5_tte_scores_topp: \", np.mean(t5_tte_scores_topp))\n",
    "print(\"bart_tte_scores: \", np.mean(bart_tte_scores))\n",
    "print(\"bart_tte_scores_topk: \", np.mean(bart_tte_scores_topk))\n",
    "print(\"bart_tte_scores_topp: \", np.mean(bart_tte_scores_topp))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"*** Grammar Scores ***\")\n",
    "#Grammar Scores\n",
    "print(\"t5_g_scores: \", np.mean(t5_g_scores))\n",
    "print(\"t5_g_scores_topk: \", np.mean(t5_g_scores_topk))\n",
    "print(\"t5_g_scores_topp: \", np.mean(t5_g_scores_topp))\n",
    "print(\"bart_g_scores: \", np.mean(bart_g_scores))\n",
    "print(\"bart_g_scores_topk: \", np.mean(bart_g_scores_topk))\n",
    "print(\"bart_g_scores_topp: \", np.mean(bart_g_scores_topp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
