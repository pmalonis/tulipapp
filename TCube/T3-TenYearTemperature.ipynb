{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import math\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/py36/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/studio-lab-user/sagemaker-studiolab-notebooks/TCube')\n",
    "import segmentation\n",
    "import utils\n",
    "import data2graph\n",
    "from finetuned import T5FineTuner, BARTFineTuner, generate, generate_beam, graph2text_nobeam, graph2text_nobeam_ngram_es, graph2text_nobeam_topk, graph2text_nobeam_topp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textstat\n",
    "import language_tool_python\n",
    "from lexical_diversity import lex_div as ld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Fine-Tuned PLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5FineTuner(\n",
       "  (model): T5ForConditionalGeneration(\n",
       "    (shared): Embedding(32103, 512)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32103, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 8)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32103, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 8)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 8)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=512, out_features=32103, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "#cuda1 = torch.device(\"cuda:1\")\n",
    "#cuda3 = torch.device(\"cuda:3\")\n",
    "\n",
    "t5 = T5FineTuner.load_from_checkpoint(\"Finetune PLMs/checkpoints/val_avg_bleu=0.0000-step_count=0.ckpt\")\n",
    "#bart = BARTFineTuner.load_from_checkpoint(\"BARTModels/BARTBoth.ckpt\")\n",
    "\n",
    "t5.to(cuda0)\n",
    "#bart.to(cuda0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[     year  TenYearAverageTemperature        Country\n",
       " 0    1834                   8.671000  United States\n",
       " 1    1835                   8.528275  United States\n",
       " 2    1836                   8.426767  United States\n",
       " 3    1837                   8.315008  United States\n",
       " 4    1838                   8.185025  United States\n",
       " ..    ...                        ...            ...\n",
       " 174  2008                   9.689717  United States\n",
       " 175  2009                   9.646725  United States\n",
       " 176  2010                   9.632308  United States\n",
       " 177  2011                   9.608742  United States\n",
       " 178  2012                   9.653233  United States\n",
       " \n",
       " [179 rows x 3 columns],      year  TenYearAverageTemperature         Country\n",
       " 0    1834                   8.613408  United Kingdom\n",
       " 1    1835                   8.556683  United Kingdom\n",
       " 2    1836                   8.429933  United Kingdom\n",
       " 3    1837                   8.367342  United Kingdom\n",
       " 4    1838                   8.173550  United Kingdom\n",
       " ..    ...                        ...             ...\n",
       " 174  2008                   9.608658  United Kingdom\n",
       " 175  2009                   9.587658  United Kingdom\n",
       " 176  2010                   9.489125  United Kingdom\n",
       " 177  2011                   9.565908  United Kingdom\n",
       " 178  2012                   9.501683  United Kingdom\n",
       " \n",
       " [179 rows x 3 columns],      year  TenYearAverageTemperature Country\n",
       " 0    1834                  12.714067  France\n",
       " 1    1835                  12.627383  France\n",
       " 2    1836                  12.586725  France\n",
       " 3    1837                  12.531867  France\n",
       " 4    1838                  12.400517  France\n",
       " ..    ...                        ...     ...\n",
       " 174  2008                  13.893508  France\n",
       " 175  2009                  13.897183  France\n",
       " 176  2010                  13.811992  France\n",
       " 177  2011                  13.890300  France\n",
       " 178  2012                  13.856200  France\n",
       " \n",
       " [179 rows x 3 columns],      year  TenYearAverageTemperature Country\n",
       " 0    1834                  13.540617   Spain\n",
       " 1    1835                  13.475267   Spain\n",
       " 2    1836                  13.450175   Spain\n",
       " 3    1837                  13.434367   Spain\n",
       " 4    1838                  13.391608   Spain\n",
       " ..    ...                        ...     ...\n",
       " 174  2008                  14.537292   Spain\n",
       " 175  2009                  14.597450   Spain\n",
       " 176  2010                  14.575175   Spain\n",
       " 177  2011                  14.645675   Spain\n",
       " 178  2012                  14.638883   Spain\n",
       " \n",
       " [179 rows x 3 columns],      year  TenYearAverageTemperature Country\n",
       " 0    1834                  12.559425   Italy\n",
       " 1    1835                  12.464725   Italy\n",
       " 2    1836                  12.423858   Italy\n",
       " 3    1837                  12.329433   Italy\n",
       " 4    1838                  12.211700   Italy\n",
       " ..    ...                        ...     ...\n",
       " 174  2008                  13.840400   Italy\n",
       " 175  2009                  13.869392   Italy\n",
       " 176  2010                  13.789333   Italy\n",
       " 177  2011                  13.811225   Italy\n",
       " 178  2012                  13.825725   Italy\n",
       " \n",
       " [179 rows x 3 columns],      year  TenYearAverageTemperature  Country\n",
       " 0    1834                   8.077442  Germany\n",
       " 1    1835                   8.002175  Germany\n",
       " 2    1836                   7.955408  Germany\n",
       " 3    1837                   7.870533  Germany\n",
       " 4    1838                   7.674500  Germany\n",
       " ..    ...                        ...      ...\n",
       " 174  2008                   9.539067  Germany\n",
       " 175  2009                   9.512908  Germany\n",
       " 176  2010                   9.311767  Germany\n",
       " 177  2011                   9.384683  Germany\n",
       " 178  2012                   9.343667  Germany\n",
       " \n",
       " [179 rows x 3 columns],      year  TenYearAverageTemperature Country\n",
       " 0    1834                  -5.903542  Russia\n",
       " 1    1835                  -6.038942  Russia\n",
       " 2    1836                  -6.116117  Russia\n",
       " 3    1837                  -6.288458  Russia\n",
       " 4    1838                  -6.319308  Russia\n",
       " ..    ...                        ...     ...\n",
       " 174  2008                  -4.085700  Russia\n",
       " 175  2009                  -4.063575  Russia\n",
       " 176  2010                  -4.061067  Russia\n",
       " 177  2011                  -3.952158  Russia\n",
       " 178  2012                  -3.930042  Russia\n",
       " \n",
       " [179 rows x 3 columns]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/studio-lab-user/.conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/studio-lab-user/.conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/studio-lab-user/.conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/studio-lab-user/.conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/studio-lab-user/.conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/studio-lab-user/.conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Import Land Temp Dataset\n",
    "ds_gtemp = pd.read_csv(\"Data/GlobalTemperature/GlobalLandTemperaturesByCountry.csv\")\n",
    "\n",
    "countries = ['United States', 'United Kingdom', 'France', 'Spain', 'Italy', 'Germany', 'Russia']\n",
    "all_dfs = []\n",
    "for c in countries:\n",
    "    country_df = ds_gtemp.loc[ds_gtemp['Country']==c]\n",
    "    country_df['dt']=pd.to_datetime(country_df['dt'])\n",
    "    rolling_df = (country_df[(country_df['dt'].dt.month<=12) \n",
    "                            & (country_df['dt'].dt.year>=1825) \n",
    "                            & (country_df['dt'].dt.year<=2012)]\n",
    "    .groupby(country_df['dt'].dt.year)[['AverageTemperature']].mean()\n",
    "    .rolling(10).mean()\n",
    "    .dropna()\n",
    "    .reset_index()\n",
    "    .rename(columns={'dt':'year', 'AverageTemperature': 'TenYearAverageTemperature'}))\n",
    "    rolling_df['Country'] = c\n",
    "    all_dfs.append(rolling_df)\n",
    "\n",
    "ds_gtemp = pd.concat(all_dfs, axis=0)\n",
    "ds_gtemp = ds_gtemp.dropna()\n",
    "ds_gtemp['month'] = 'July'\n",
    "ds_gtemp['day'] = 1\n",
    "ds_gtemp['dt'] = pd.to_datetime(ds_gtemp[['year', 'month', 'day']].astype(str).agg('-'.join, axis=1),format='%Y-%B-%d')\n",
    "ds_gtemp.set_index(['dt'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Country:  United States\n",
      "\n",
      " Data Loaded\n",
      "\n",
      " Waves Detected\n",
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/TCube/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Generation Complete:  United States\n",
      "Top-k Complete:  United States\n",
      "Top-p Complete:  United States\n",
      "RE Scores Computed:  United States\n",
      "TTE Scores Computed:  United States\n",
      "Processing Country:  United Kingdom\n",
      "\n",
      " Data Loaded\n",
      "\n",
      " Waves Detected\n",
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/TCube/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Generation Complete:  United Kingdom\n",
      "Top-k Complete:  United Kingdom\n",
      "Top-p Complete:  United Kingdom\n",
      "RE Scores Computed:  United Kingdom\n",
      "TTE Scores Computed:  United Kingdom\n",
      "Processing Country:  France\n",
      "\n",
      " Data Loaded\n",
      "\n",
      " Waves Detected\n",
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/TCube/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Generation Complete:  France\n",
      "Top-k Complete:  France\n",
      "Top-p Complete:  France\n",
      "RE Scores Computed:  France\n",
      "TTE Scores Computed:  France\n",
      "Processing Country:  Spain\n",
      "\n",
      " Data Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/TCube/utils.py:80: RuntimeWarning: divide by zero encountered in log\n",
      "  log_std = np.log(np.std(data[entry[0]:entry[1]]))\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/TCube/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Waves Detected\n",
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n",
      "Simple Generation Complete:  Spain\n",
      "Top-k Complete:  Spain\n",
      "Top-p Complete:  Spain\n",
      "RE Scores Computed:  Spain\n",
      "TTE Scores Computed:  Spain\n",
      "Processing Country:  Italy\n",
      "\n",
      " Data Loaded\n",
      "\n",
      " Waves Detected\n",
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/TCube/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Generation Complete:  Italy\n",
      "Top-k Complete:  Italy\n",
      "Top-p Complete:  Italy\n",
      "RE Scores Computed:  Italy\n",
      "TTE Scores Computed:  Italy\n",
      "Processing Country:  Germany\n",
      "\n",
      " Data Loaded\n",
      "\n",
      " Waves Detected\n",
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/TCube/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Generation Complete:  Germany\n",
      "Top-k Complete:  Germany\n",
      "Top-p Complete:  Germany\n",
      "RE Scores Computed:  Germany\n",
      "TTE Scores Computed:  Germany\n",
      "Processing Country:  Russia\n",
      "\n",
      " Data Loaded\n",
      "\n",
      " Waves Detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/TCube/segmentation.py:210: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  kmeans_results = cluster.KMeans(n_clusters=k, tol=tolerance).fit(embeddings)\n",
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/TCube/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n",
      "Simple Generation Complete:  Russia\n",
      "Top-k Complete:  Russia\n",
      "Top-p Complete:  Russia\n",
      "RE Scores Computed:  Russia\n",
      "TTE Scores Computed:  Russia\n"
     ]
    }
   ],
   "source": [
    "#RE Scores\n",
    "template_re_scores = []\n",
    "t5_re_scores = []\n",
    "t5_re_scores_topk = []\n",
    "t5_re_scores_topp = []\n",
    "#bart_re_scores = []\n",
    "#bart_re_scores_topk = []\n",
    "#bart_re_scores_topp = []\n",
    "\n",
    "#Diveristy Scores\n",
    "template_tte_scores = []\n",
    "t5_tte_scores = []\n",
    "t5_tte_scores_topk = []\n",
    "t5_tte_scores_topp = []\n",
    "#bart_tte_scores = []\n",
    "#bart_tte_scores_topk = []\n",
    "#bart_tte_scores_topp = []\n",
    "\n",
    "#Grammar Scores\n",
    "t5_g_scores = []\n",
    "t5_g_scores_topk = []\n",
    "t5_g_scores_topp = []\n",
    "#bart_g_scores = []\n",
    "#bart_g_scores_topk = []\n",
    "#bart_g_scores_topp = []\n",
    "\n",
    "#Grammar Mistakes\n",
    "t5_g_mistake = []\n",
    "t5_g_mistake_topk = []\n",
    "t5_g_mistake_topp = []\n",
    "#bart_g_mistake = []\n",
    "#bart_g_mistake_topk = []\n",
    "#bart_g_mistake_topp = []\n",
    "\n",
    "#countries = ['United States', 'India', 'Brazil', 'Russia', 'United Kingdom', 'France', 'Spain', 'Italy' , 'Turkey', 'Germany']\n",
    "\n",
    "all_t5_narrative = []\n",
    "all_t5_narrative_topk = []\n",
    "all_t5_narrative_topp = []\n",
    "all_country = []\n",
    "all_country_gtemp_raw = []\n",
    "\n",
    "for c in countries:\n",
    "    \n",
    "    print(\"Processing Country: \", c)\n",
    "    \n",
    "    country = ds_gtemp[ds_gtemp['Country']==c][['TenYearAverageTemperature','year', 'month']].reset_index().drop(columns=['dt'])\n",
    "    country_gtemp_raw = country['TenYearAverageTemperature'].tolist()\n",
    "    \n",
    "    all_country.append(country)\n",
    "    all_country_gtemp_raw.append(country_gtemp_raw)\n",
    "    \n",
    "    #Log-normalize data\n",
    "    trans = np.ma.log(country_gtemp_raw)\n",
    "    country_gtemp = trans.filled(0)\n",
    "    \n",
    "    print(\"\\n Data Loaded\")\n",
    "    \n",
    "    #Detecting Waves\n",
    "    embeds, cluster_labels = segmentation.tslr_rep(country_gtemp)\n",
    "    cluster_arrangement = utils.find_contiguous(cluster_labels)\n",
    "    indices = utils.find_indices(cluster_arrangement)\n",
    "    wave_indices = utils.find_waves(country_gtemp_raw, indices, tolerance=3)\n",
    "    \n",
    "    print(\"\\n Waves Detected\")\n",
    "\n",
    "    #Detecting Trends\n",
    "    segmentation_results = segmentation.sliding_window(country_gtemp, 4)\n",
    "    print(\"\\n Segmentation Done\")\n",
    "    filtered_results = segmentation.re_segment(segmentation_results, country_gtemp)\n",
    "    trends = segmentation.find_trend(filtered_results, country_gtemp)\n",
    "    \n",
    "    print(\"\\n Trends Detected\")\n",
    "    \n",
    "    location = c\n",
    "    \n",
    "    graph, essentials = data2graph.build_graph_gtemp_form1(\"Average Temperature\", location, wave_indices, trends, country, country_gtemp_raw )\n",
    "    \n",
    "    print(\"\\n Graph Calculated\")\n",
    "    \n",
    "    #Template Narrative\n",
    "    template_text = data2graph.build_template_gtemp_nums(\"Average Temperature\", location, wave_indices, trends, country, country_gtemp_raw )\n",
    "    \n",
    "    print(\"\\n Templated Computed\")\n",
    "    \n",
    "    t5_prefix = 'translate Graph to English: '\n",
    "    \n",
    "    iso = c\n",
    "    \n",
    "    #Simple PLM Generation\n",
    "    t5_narrative = graph2text_nobeam(t5, graph, t5_prefix, 512, cuda0)\n",
    "    all_t5_narrative.append(t5_narrative)\n",
    "    #bart_narrative = graph2text_nobeam(bart , graph, \"\", 512, cuda0)\n",
    "    #bart_narrative = re.sub('</s>' , '', bart_narrative)\n",
    "    \n",
    "    print(\"Simple Generation Complete: \", iso)\n",
    "    \n",
    "    #Top-k at 50\n",
    "    t5_narrative_topk = graph2text_nobeam_topk(t5, graph, t5_prefix, 50, 512, cuda0)\n",
    "    all_t5_narrative_topk.append(t5_narrative_topk)\n",
    "    #bart_narrative_topk = graph2text_nobeam_topk(bart, graph, \"\", 50, 512, cuda0)\n",
    "    #bart_narrative_topk = re.sub('</s>' , '', bart_narrative_topk)\n",
    "    \n",
    "    print(\"Top-k Complete: \", iso)\n",
    "    \n",
    "    #Top-p at 0.92\n",
    "    t5_narrative_topp = graph2text_nobeam_topp(t5, graph, t5_prefix, 0.92, 512, cuda0)\n",
    "    all_t5_narrative_topp.append(t5_narrative_topp)\n",
    "    #bart_narrative_topp = graph2text_nobeam_topp(bart, graph, \"\", 0.92, 512, cuda0)\n",
    "    #bart_narrative_topp = re.sub('</s>' , '', bart_narrative_topp)\n",
    "    \n",
    "    print(\"Top-p Complete: \", iso)\n",
    "    \n",
    "    #RE Scores\n",
    "    template_re_scores.append(textstat.flesch_reading_ease(template_text))\n",
    "    t5_re_scores.append(textstat.flesch_reading_ease(t5_narrative))\n",
    "    t5_re_scores_topk.append(textstat.flesch_reading_ease(t5_narrative_topk))\n",
    "    t5_re_scores_topp.append(textstat.flesch_reading_ease(t5_narrative_topp))\n",
    "    #bart_re_scores.append(textstat.flesch_reading_ease(bart_narrative))\n",
    "    #bart_re_scores_topk.append(textstat.flesch_reading_ease(bart_narrative_topk))\n",
    "    #bart_re_scores_topp.append(textstat.flesch_reading_ease(bart_narrative_topp))\n",
    "    \n",
    "    print(\"RE Scores Computed: \", iso)\n",
    "    \n",
    "    #Diveristy Scores\n",
    "    template_tte_scores.append(ld.ttr(ld.flemmatize(template_text)))\n",
    "    t5_tte_scores.append(ld.ttr(ld.flemmatize(t5_narrative)))\n",
    "    t5_tte_scores_topk.append(ld.ttr(ld.flemmatize(t5_narrative_topk)))\n",
    "    t5_tte_scores_topp.append(ld.ttr(ld.flemmatize(t5_narrative_topp)))\n",
    "    #bart_tte_scores.append(ld.ttr(ld.flemmatize(bart_narrative)))\n",
    "    #bart_tte_scores_topk.append(ld.ttr(ld.flemmatize(bart_narrative_topk)))\n",
    "    #bart_tte_scores_topp.append(ld.ttr(ld.flemmatize(bart_narrative_topp)))\n",
    "    \n",
    "    print(\"TTE Scores Computed: \", iso)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The average temperature in the United States was taken in 1834. The temperature for the species with a 9.65 celsius is from 1834 to 2012. Average temperature observed in 2007 at 9.81 g of the average temperature. ',\n",
       " 'Average temperatures are at the United Kingdom, and the first measurements are taken in 1834. With a first average for the year lasted of 1912 to 1934, climaxed at 8.55 celsius. Temperaturs from 1834 to 2012 were increased to 9.5 celsius The average temperature was at 9.62 in 2007. ',\n",
       " 'The average temperature in France was taken in 1834. The temperature between 1834 to 2012 was 13.86 Celsius. The Average Temperature Average Temperature Peak is 13.93 in 2006. ',\n",
       " 'The average temperature in Spain was taken in 1834. The first average period in the period 1834 to 1835 peaked at 13.54 celsius. Temperature from 1834 to 2012 at 14.64 cl. Average temperature when the observed value was 14.66 was peak in 2003. ',\n",
       " 'The average temperature in Italy was taken by the local authorities in 1834. The temperature between 1834 to 2012 is 13,83 celsius. The average temperature in 2009 grew at 13.87. ',\n",
       " 'The average temperature of Germany is 1834. The temperature from 1834 to 2012 was 0.9 The average temperature recorded was 9.54 in 2008. ',\n",
       " 'The average temperature is in Russia and the first measurement is taken in 1834. The temperature of the vehicle is -3.93. Average Temperature had an observed peak in 2012. ']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " all_t5_narrative_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The average template in Russia was first measured in July 1834. The temperature from July 1834 to July 2012 flatline to -3.93The average temperature reached its peak at -3.93 in July 2012.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The global temperature is in the United States and it was taken in 1768. The temperature from 1768 to 1969 increased to 2.91 degrees Celsius. The temperature from 1969 to 1977 was -2.19 degrees Celsius. The temperature from 1980 to 1983 was 21.61 degrees Celsius. The temperature from 2000 to 2013 was 14.07 degrees Celsius and the temperature was 6.97 degrees Celsius. The temperature of the year 2013 to 2013 was sharp increase to 18.27 celsius. Global Temperature's peak was 23.01 in 2012. \""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "df = all_country[0]\n",
    "df['day'] = 1\n",
    "df = df.set_index(pd.to_datetime(df[['year', 'month', 'day']].astype(str).agg('-'.join, axis=1),format='%Y-%B-%d'))\n",
    "df.plot(y='AverageTemperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Country:  United States\n",
      "\n",
      " Data Loaded\n",
      "\n",
      " Waves Detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n",
      "Simple Generation Complete:  United States\n",
      "Top-k Complete:  United States\n",
      "Top-p Complete:  United States\n",
      "RE Scores Computed:  United States\n",
      "TTE Scores Computed:  United States\n",
      "Grammar Scores Computed:  United States\n",
      "Processing Country:  India\n",
      "\n",
      " Data Loaded\n",
      "\n",
      " Waves Detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n",
      "Simple Generation Complete:  India\n",
      "Top-k Complete:  India\n",
      "Top-p Complete:  India\n",
      "RE Scores Computed:  India\n",
      "TTE Scores Computed:  India\n",
      "Grammar Scores Computed:  India\n",
      "Processing Country:  Brazil\n",
      "\n",
      " Data Loaded\n",
      "\n",
      " Waves Detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n",
      "Simple Generation Complete:  Brazil\n",
      "Top-k Complete:  Brazil\n",
      "Top-p Complete:  Brazil\n",
      "RE Scores Computed:  Brazil\n",
      "TTE Scores Computed:  Brazil\n",
      "Grammar Scores Computed:  Brazil\n",
      "Processing Country:  Russia\n",
      "\n",
      " Data Loaded\n",
      "\n",
      " Waves Detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n",
      "Simple Generation Complete:  Russia\n",
      "Top-k Complete:  Russia\n",
      "Top-p Complete:  Russia\n",
      "RE Scores Computed:  Russia\n",
      "TTE Scores Computed:  Russia\n",
      "Grammar Scores Computed:  Russia\n",
      "Processing Country:  United Kingdom\n",
      "\n",
      " Data Loaded\n",
      "\n",
      " Waves Detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n",
      "Simple Generation Complete:  United Kingdom\n",
      "Top-k Complete:  United Kingdom\n",
      "Top-p Complete:  United Kingdom\n",
      "RE Scores Computed:  United Kingdom\n",
      "TTE Scores Computed:  United Kingdom\n",
      "Grammar Scores Computed:  United Kingdom\n",
      "Processing Country:  France\n",
      "\n",
      " Data Loaded\n",
      "\n",
      " Waves Detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n",
      "Simple Generation Complete:  France\n",
      "Top-k Complete:  France\n",
      "Top-p Complete:  France\n",
      "RE Scores Computed:  France\n",
      "TTE Scores Computed:  France\n",
      "Grammar Scores Computed:  France\n",
      "Processing Country:  Spain\n",
      "\n",
      " Data Loaded\n",
      "\n",
      " Waves Detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n",
      "Simple Generation Complete:  Spain\n",
      "Top-k Complete:  Spain\n",
      "Top-p Complete:  Spain\n",
      "RE Scores Computed:  Spain\n",
      "TTE Scores Computed:  Spain\n",
      "Grammar Scores Computed:  Spain\n",
      "Processing Country:  Italy\n",
      "\n",
      " Data Loaded\n",
      "\n",
      " Waves Detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n",
      "Simple Generation Complete:  Italy\n",
      "Top-k Complete:  Italy\n",
      "Top-p Complete:  Italy\n",
      "RE Scores Computed:  Italy\n",
      "TTE Scores Computed:  Italy\n",
      "Grammar Scores Computed:  Italy\n",
      "Processing Country:  Turkey\n",
      "\n",
      " Data Loaded\n",
      "\n",
      " Waves Detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n",
      "Simple Generation Complete:  Turkey\n",
      "Top-k Complete:  Turkey\n",
      "Top-p Complete:  Turkey\n",
      "RE Scores Computed:  Turkey\n",
      "TTE Scores Computed:  Turkey\n",
      "Grammar Scores Computed:  Turkey\n",
      "Processing Country:  Germany\n",
      "\n",
      " Data Loaded\n",
      "\n",
      " Waves Detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Segmentation Done\n",
      "\n",
      " Trends Detected\n",
      "\n",
      " Graph Calculated\n",
      "\n",
      " Templated Computed\n",
      "Simple Generation Complete:  Germany\n",
      "Top-k Complete:  Germany\n",
      "Top-p Complete:  Germany\n",
      "RE Scores Computed:  Germany\n",
      "TTE Scores Computed:  Germany\n",
      "Grammar Scores Computed:  Germany\n"
     ]
    }
   ],
   "source": [
    "    #Grammar Scores\n",
    "    gs = grammar_score(t5_narrative)\n",
    "    t5_g_scores.append(gs)\n",
    "    if gs != 1.0:\n",
    "        t5_g_mistake.append((graph, t5_narrative))\n",
    "    \n",
    "    gs = grammar_score(t5_narrative_topk)\n",
    "    t5_g_scores_topk.append(gs)\n",
    "    if gs != 1.0:\n",
    "        t5_g_mistake_topk.append((graph, t5_narrative_topk))\n",
    "    \n",
    "    gs = grammar_score(t5_narrative_topp)\n",
    "    t5_g_scores_topp.append(gs)\n",
    "    if gs != 1.0:\n",
    "        t5_g_mistake_topp.append((graph, t5_narrative_topp))\n",
    "    \n",
    "    gs = grammar_score(bart_narrative)                          \n",
    "    bart_g_scores.append(gs)\n",
    "    if gs != 1.0:\n",
    "        bart_g_mistake.append((graph, bart_narrative))\n",
    "        \n",
    "    gs = grammar_score(bart_narrative_topk)\n",
    "    bart_g_scores_topk.append(gs)\n",
    "    if gs != 1.0:\n",
    "        bart_g_mistake_topk.append((graph, bart_narrative_topk))\n",
    "    \n",
    "    gs = grammar_score(bart_narrative_topp)\n",
    "    bart_g_scores_topp.append(gs)\n",
    "    if gs != 1.0:\n",
    "        bart_g_mistake_topp.append((graph, bart_narrative_topp))\n",
    "    \n",
    "    print(\"Grammar Scores Computed: \", iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = {\n",
    "    'countries': countries,\n",
    "    'all_t5_narrative': all_t5_narrative,\n",
    "    'all_t5_narrative_topk': all_t5_narrative_topk,\n",
    "    'all_t5_narrative_topp': all_t5_narrative_topp,\n",
    "    'all_country': all_country,\n",
    "    'all_country_gtemp_raw': all_country_gtemp_raw\n",
    "}\n",
    "\n",
    "# Save the data to a pickle file\n",
    "with open('t5-small_evaluation_results.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The first measurement of the Global Temperature took place in the US in 1768. From 1768 to 1969, to 8.89 degrees Celsius, the temperature increased to 2.91 degrees Celsius. The temperature between 1969 and 1980 was -2.19. There were a temperature increased to 9.23 in 1980 to 2000 and a temperature that drew to 21.61 in 2000. In from 2000 to 2013 the temperature was 14.07 the temperature was 6.97 the temperature was sharp. The temperature during 2013 to 2013 was on the rise with a sharp increase of to 18.27 celsius. Global Temperature reached 23.01 in 2012. ',\n",
       " 'Global Temperature took place in India in 1796. The temperature from 1796 to 1816 and temperature from 1816 to 1849 were decreased to 29.51. the temperature from 1849 to 1866 increased to 19.91 degrees C. The temperature of 83 degrees Celsius for the period between 1897 and 1976 was 17.04. In the years 1977 to 2013, the temperature when the temperature is 10cf is 26.55 is 16.24. Global Temperature peak was observed in 1921 at 31.33. ',\n",
       " 'The first measurement for the Global Temperature is in Brazil is 1832. The temperature from 1832 to 2013 was 24.74 Celsius. Global Temperature has a peak observed in 2009. ',\n",
       " 'The Global Temperature occurred in Russia in 1812. The temperature between 1812 and 2013 is 13.82 celsius. The global temperature was 16.89 in 2010. ',\n",
       " 'The event with the first measurement taken in 1743 is located in the United Kingdom. There was a sharp increase of 3.19 degrees Celsius between 1743 to 1760 and 0.65 degrees Celsius. Temperature from 1761 to 1771 to 3.0 was 11.34 Celsius. Temperatures that were from 1771 to 1776 increased to -1.22 (Celebrates) decreased to 4.11 (Celebrates) From 1776 to 1782, the temperature increased to 2.73 and decreased to 3.49 Celsius. From 1799 to 1806 the temperature increased to 4.28 Celsius from 1806 to 1809. The temperature of most lupuus heated was 2.18 to 5.51 degrees. The temperature of 1815 to 1816 was decreased to 10.6 degrees Celsius and increased to 3.81 degrees Celsius Celsius. The temperature from 1819 to 1826 was 5,94. The temperature range of the altitude from 1826 to 1827 was 0.8 and decreased to 14.83 degrees C. The temperature of the Earth from 1829 to 1832 was 6.4 and the temperature weighed 12.93 degrees. The temperature increased to 12.41 of the Cabria falls to 13.92 at 1843 to 1848 The temperature of 1848 to 1853 on the planet decreased to 3.19 celsius, decreasing to 5.08 celsius The temperature for 1856 to 1864 was 4.09 Celsius. Temperatures from 1866 to 1879 increased to 14.91 Celsius by 14.91 Celsius and 2.94 Celsius. The temperature of 1884 to 1888 with the increases to 3.56 Celsius was 2.16 (Cleasius). Temperature from 1890 to 1895 was -0.69 celsius and grew to 15.13 celsius. The temperature of the houses from 1899 to 1903 is 12.64 and the temperature in 1903 to 1906 is 3.22. The temperature of a birth site in 1911 to 1906 increased to 4.11 Celsius to 13.95 Celsius. The temperature from 1911 to 1914 increased to 4.87 degrees Celsius. The temperature between 1928 to 1942 was 1.07 and 12,09. The temperature in which CBD stands from 1945 to 1959 has increased to 2.64 to 4.76. From 1962 to 1963 it decreased to 0.46 degrees Celsius and increased to 4.55 degrees Celsius. The temperature that was increased to 15.51 degrees Fahrenheit fell to 13.09 degrees Fahrenheit from 1968 to 1976 The temperature range from 1976 to 1979 was 16.592 Celsius. In comparison to 1988, the temperature was 13.24. Temperatures from 1988 to 1991 decreased to 15.28 Celsius and increased to 4.51 Celsius Celsius. The temperature from 1995 to 2004 was 14.2, while the temperature was 5.23C. On the 1998-14 season the temperature was 15.02 cl (calf). Global Temperature is at 17.29 feet above sea level. ',\n",
       " \"the global temperature is in France and the first measurement is taken in 1743. The temperature from 1743 to 1758 decreased to 6.57 Celsius and the temperature from 1758 to 1761 increased to 4.51 Celsius. The temperature from 1761 to 1772 was 8.94 and increased to 19.89 degrees Celsius. In 1779 to 1797 temperature increased to 16.61 to 3.87 The temperature from 1797 to 1807 at the time was 7.7 and the temperature was 8.02C. From 1812 to 1814 the temperature fell to 7.12 degrees Celsius, and was reduced to 18.73 degrees Celsius. The temperature increased to 16.01 and the temperature decreased to 18.77 in 1837. Temperatures ranging from 1839 to 1848 increased to 17.76 degrees Celsius and decreased to 3.9 degrees Celsius. The temperature increased to 9.97 degrees. was between 1858 and 1860. The temperature of 1867 to 1872 increased to 5.23 degrees Celsius. The temperature between 1876 to 1879 was 5.68 degrees Celsius and decreased to 17.42 degrees Celsius. The temperature from 1886 to 1891 is 7.59. The temperature of the city whose temperature is 8.92 with the 1915 to 1915 years is 18.72 and the date of the 1925 to 1923 census is 1904. Temperature from 1923 to 1930 increased to 7.16 degrees. From 1938 to 1941, the temperature in the years of 1941 to 1961 was 6.83. The temperature of Cretus from 1961 to 1963 went to 20.6 degrees Celsius. The temperature between 1979 and 1983 from 1980 was 6.61C. Temperatures from 1984 to 1986, increased to 20.82, and decreased to 19.85 C. From 1994 to 1999, the temperature was 27 degrees Celsius and the temperature was 20.37 degrees Celsius. The Global Temperature's peak in 2003 at 23.66 \",\n",
       " 'The world temperature is in Spain and the first measurement is taken 1743. The temperature of the Icelandic sun service from 1743 to 1751 was 20.96 C. Temperature from 1756 to 1764 sunk to 9.41 degrees Celsius and to 1.72 degrees Celsius The temperature in 1766 to 1770 increased to 19.43. The temperature in that night was decreased to 18.98. The temperature from 1773 to 1793 increased to 8.02 degrees Celsius. The temperature between 1793 to 1798 was 22.05 and 23.29 degrees Fahrenheit. In 1808 to 1813 the temperature was 4.3 degrees Celsius and decreased to 15.34 degrees Celsius. Temperatures of 1815 to 1818 increased to 6.87 degrees Fahrenheit and decreased to 22.39 degrees Fahrenheit The temperature of December 1833 to April 1842 was at 23.71 degrees Celsius. The temperature when if in the period of 1842 to 1847 is 20.24 cl is 8.61 cl. The temperature between 1849 to 1859 is 6.98 cl. and 22.9 cl. The temperature between 1873 to 1875 was 2.98 degrees Fahrenheit and 22.73 degrees Fahrenheit Temperature from 1883 to 1907 increased to 5.09 degrees Celsius from 1907 to 1912. Temperature from 1912 to 1917 is down to 9.94 degrees Celsius from 1917 to 1919 and decreased to 23.79 degrees Celsius. From 1919 to 1922, Temperatures increased to 18.64 Celsius in 1922 to 1940 and Temperature increased to 22.83 Celsius in 1922 The temperature in 1745 to 2730 degrees F is 26.59 degrees F Temperatures from 1947 to 1961 increased to 9.99 Celsius and to 18.16 Celsius. Between 1963 to 1966, the temperature to it increased to 16.2 Celsius and fell to 22.52 Celsius. The temperature increased to 21.33 degrees Celsius, from 1980 to 1983, down to 5.77 degrees Celsius. Temperatures from 1983 to 1994 decreased to 24.22, and from 1994 to 2001 to 8.3. Temperatures from 2001 to 2004 were 5.31 and 7.88 to 2010. The temperature between 2009 and 2012 increased to 24.02. The global temperature was 26.03 in 1761. ',\n",
       " 'The atmosphere at the Global Temperature was taken in Italy in 1743. During the season 1743 to 1750 a temperature that climbed to 1.83 was 21.85. The temperature of 1753 to 1754 is 1.56 and 7.02. Temperatures from 1756 to 1760 decreased to 3.78 in Celsius and to 6.24 in Celsius. The temperature of those in 1767 to 1774 increased to 5.83 and decreased to 1.09, respectively. The temperature from 1777 to 1786 in Cleave got an average of 4.89 degrees Celsius. The temperature from 1786 to 1807 is 23.45 and the temperature is 6.61 degrees Celsius. The temperature for the period ranging from 1807 to 1808 increased to 22.91 is 22.92 latteus. The temperature from 1811 to 1816 is 3.04 degrees Fahrenheit, and the temperature is increased to 19.12 degrees Fahrenheit. Temperatures from 1826 to 1833 increased to 16.2 Celsius by 22.06 celsius Temperatures from 1835 to 1836 increased to 2.69 on the celsius scale to 22.3 on the celsius scale. The temperature from 1838 to 1847 was reduced to 5.46. From 1859 to 1860, the temperature was 5.89 degrees Celsius. The temperature of 1866 to 1871 increased to 21.25, and to 20.93 Celsius. The temperature during 1872 to 1879 was 7.18. The temperature from 1879 to 1887 increased to 21.93 degrees Celsius and the temperature was 16.17 degrees Celsius. The temperature from 1890 to 1893 compared to the temperature of 4.56 below celsius. Temperature from 1896 to 1904 was 6.55 at temperatures increased to 19.12 and number of cleaves The temperature from 1904 to 1907 was 3.42 at the same time and it decreased to 5.36 at the same time. The temperature from 1935 to 1944 decreased to 18.63 degrees Fahrenheit and increased to 23.26 degrees Fahrenheit. From 1944 to 1951, the temperature increased to 20.01 celsius and the temperature increased to 2.94 celsius. During the decades of 1953 to 1961, temperature increased to 4.3 Celsius. The temperature for the year 1976 to 1972 was 5.68. The temperature in the range of 1977 to 1985 was 23.6 degreesC in 2013, up from 5.05 degreesC. Global Temperature has the highest observed value in 2003 at 26.12. ',\n",
       " 'The Turkey U.S. Global Temperature is 175 (317). Temperatures from 1777 to 1784 remained relatively low and increased to 3.23 degrees Celsius. The temperature between 1784 to 1815 increased to 12.06, putting the temperature at 3.26C. The temperature of 1825 to 1831 was 4.5 Celsius. Between 1836 to 1838, temperature which has increased to 7.59, -3.71 (CEL). The temperature at the time when temperature was between 1839 and 1844 was 3.69 was 23.41. Temperature from 1860 to 1862 and temperature from 1877 to 1877 increased to 1.73 degreesC. The temperature of the first conifer between 1877 and 1916, was 2.51 and 1.8. The temperature of both the world and the family are 12.63 c.(b)s. Between 1928 to 1935, there were 2.6 celsius and 2.34 celsius (where) Temperature of -1.56 celsius in 1962 to 1981 increased to 3.44 celsius. During the period from 1981 to 1983, the temperature to Souvius from 1983 to 1992 was 14.98. From 1992 to 2008 the temperature increased to -2.35 celsius. The temperature from 2008 to 2011 when the temperature increased to 2.46 is 23.84 celsius. Global Temperature peak in 2010 at 26.27 ',\n",
       " 'The first measurement in Germany was taken in 1743. The temperature ranging from 1743 to 1744 was 0.06 and increased to 4.3 for Celsius. The temperature of 1752 to 1770 is 1,87, while the temperature is 14.84. The temperature between 1770 and 1787 went to -0.69 degrees Celsius and added to 2.13 degrees Celsius. During 1787 to 1806, the temperature to celsius increased to 0.62 degrees Celsius and it increased to 4.69 degrees Celsius. The temperature when the temperature was 1806 to 1824 was 1.48 it increased to 1.67. The temperature between 1831 to 1835 increased to 6.58 degrees C and a sharp decrease to 10.06 degrees C. The temperature from 1836 to 1838 at the time is -8.76 degrees Celsius, it is 9.49 degrees Celsius. The temperature from 1859 to 1883 increased to 7.85. The temperature between 1894 to 1919 was 5.47 and 13.48 clerbis. Temperature from 1934 to 1956 decreased to -9.3 Celsius and to 16.42 Celsius. The temperature of 1970 to 1969 fell to 0.89 celsius and was 17.05 celsius. Temperatures from 1969 to 1978 increased to -1.35 Celsius and to 8.43 Celsius. The temperature for the period between 1995 to 2003 was -0.52 (Clearus) and 8 (14) (Clearus). Temperature between 2012 to 2013 was 0.26m. The temperature range from 2013 to 2013 has a sharp increase to 18.23 degrees Celsius. Global Temperature has a peak observed in 2006. ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "data = pickle.load(open('t5-small_evaluation_results.pickle','rb'))\n",
    "data['all_t5_narrative_topp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** RE Scores ***\n",
      "template_re_scores:  -32.60000000000001\n",
      "t5_re_scores:  67.479\n",
      "t5_re_scores_topk:  66.064\n",
      "t5_re_scores_topp:  66.98\n",
      "bart_re_scores:  63.959\n",
      "bart_re_scores_topk:  64.581\n",
      "bart_re_scores_topp:  65.467\n",
      "\n",
      "\n",
      "*** Diversity Scores ***\n",
      "template_tte_scores:  0.37362918642999043\n",
      "t5_tte_scores:  0.39800298942676315\n",
      "t5_tte_scores_topk:  0.4648429017172216\n",
      "t5_tte_scores_topp:  0.4459926517363096\n",
      "bart_tte_scores:  0.4230103896435353\n",
      "bart_tte_scores_topk:  0.40254574550059957\n",
      "bart_tte_scores_topp:  0.4141072704373586\n",
      "\n",
      "\n",
      "*** Grammar Scores ***\n",
      "t5_g_scores:  0.9160556938702124\n",
      "t5_g_scores_topk:  0.962284671126657\n",
      "t5_g_scores_topp:  0.9597113481616681\n",
      "bart_g_scores:  0.9200155183341259\n",
      "bart_g_scores_topk:  0.9444883556410734\n",
      "bart_g_scores_topp:  0.935676241115152\n"
     ]
    }
   ],
   "source": [
    "#RE Scores\n",
    "print(\"*** RE Scores ***\")\n",
    "print(\"template_re_scores: \", np.mean(template_re_scores))\n",
    "print(\"t5_re_scores: \", np.mean(t5_re_scores))\n",
    "print(\"t5_re_scores_topk: \", np.mean(t5_re_scores_topk))\n",
    "print(\"t5_re_scores_topp: \", np.mean(t5_re_scores_topp))\n",
    "print(\"bart_re_scores: \", np.mean(bart_re_scores))\n",
    "print(\"bart_re_scores_topk: \", np.mean(bart_re_scores_topk))\n",
    "print(\"bart_re_scores_topp: \", np.mean(bart_re_scores_topp))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"*** Diversity Scores ***\")\n",
    "#Diveristy Scores\n",
    "print(\"template_tte_scores: \", np.mean(template_tte_scores))\n",
    "print(\"t5_tte_scores: \", np.mean(t5_tte_scores))\n",
    "print(\"t5_tte_scores_topk: \", np.mean(t5_tte_scores_topk))\n",
    "print(\"t5_tte_scores_topp: \", np.mean(t5_tte_scores_topp))\n",
    "print(\"bart_tte_scores: \", np.mean(bart_tte_scores))\n",
    "print(\"bart_tte_scores_topk: \", np.mean(bart_tte_scores_topk))\n",
    "print(\"bart_tte_scores_topp: \", np.mean(bart_tte_scores_topp))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"*** Grammar Scores ***\")\n",
    "#Grammar Scores\n",
    "print(\"t5_g_scores: \", np.mean(t5_g_scores))\n",
    "print(\"t5_g_scores_topk: \", np.mean(t5_g_scores_topk))\n",
    "print(\"t5_g_scores_topp: \", np.mean(t5_g_scores_topp))\n",
    "print(\"bart_g_scores: \", np.mean(bart_g_scores))\n",
    "print(\"bart_g_scores_topk: \", np.mean(bart_g_scores_topk))\n",
    "print(\"bart_g_scores_topp: \", np.mean(bart_g_scores_topp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36:Python",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
