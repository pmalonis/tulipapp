{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import math\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation\n",
    "import utils\n",
    "import data2graph\n",
    "from finetuned import T5FineTuner, BARTFineTuner, generate, generate_beam, graph2text_nobeam, graph2text_nobeam_ngram_es, graph2text_nobeam_topk, graph2text_nobeam_topp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "import language_tool_python\n",
    "from lexical_diversity import lex_div as ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "def grammar_score(input_text):\n",
    "    errors = len(tool.check(input_text))\n",
    "    clean_text = input_text.translate(str.maketrans('', '', string.punctuation))\n",
    "    clean_text = list(filter(None, clean_text.split(' ')))\n",
    "    num_words = len(clean_text)\n",
    "    return float(1-(errors/num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Fine-Tuned PLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BARTFineTuner(\n",
       "  (model): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): Embedding(50268, 1024)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): Embedding(50268, 1024)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): Embedding(50268, 1024)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#cuda0 = torch.device(\"cuda:0\")\n",
    "#cuda1 = torch.device(\"cuda:1\")\n",
    "cuda3 = torch.device(\"cuda:3\")\n",
    "\n",
    "t5 = T5FineTuner.load_from_checkpoint(\"T5Models/T5Both.ckpt\")\n",
    "bart = BARTFineTuner.load_from_checkpoint(\"BARTModels/BARTBoth.ckpt\")\n",
    "\n",
    "t5.to(cuda3)\n",
    "bart.to(cuda3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Population Dataset\n",
    "ds_pop = pd.read_csv(\"Data/Population/Pop.csv\")\n",
    "ds_pop = ds_pop.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Country:  USA\n",
      "Waves Detected:  USA\n",
      "Trends Detected:  USA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:140: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Generation Complete:  USA\n",
      "Top-k Complete:  USA\n",
      "Top-p Complete:  USA\n",
      "RE Scores Computed:  USA\n",
      "TTE Scores Computed:  USA\n",
      "Grammar Scores Computed:  USA\n",
      "Processing Country:  IND\n",
      "Waves Detected:  IND\n",
      "Trends Detected:  IND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:140: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Generation Complete:  IND\n",
      "Top-k Complete:  IND\n",
      "Top-p Complete:  IND\n",
      "RE Scores Computed:  IND\n",
      "TTE Scores Computed:  IND\n",
      "Grammar Scores Computed:  IND\n",
      "Processing Country:  BRA\n",
      "Waves Detected:  BRA\n",
      "Trends Detected:  BRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:140: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Generation Complete:  BRA\n",
      "Top-k Complete:  BRA\n",
      "Top-p Complete:  BRA\n",
      "RE Scores Computed:  BRA\n",
      "TTE Scores Computed:  BRA\n",
      "Grammar Scores Computed:  BRA\n",
      "Processing Country:  RUS\n",
      "Waves Detected:  RUS\n",
      "Trends Detected:  RUS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Generation Complete:  RUS\n",
      "Top-k Complete:  RUS\n",
      "Top-p Complete:  RUS\n",
      "RE Scores Computed:  RUS\n",
      "TTE Scores Computed:  RUS\n",
      "Grammar Scores Computed:  RUS\n",
      "Processing Country:  GBR\n",
      "Waves Detected:  GBR\n",
      "Trends Detected:  GBR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:140: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Generation Complete:  GBR\n",
      "Top-k Complete:  GBR\n",
      "Top-p Complete:  GBR\n",
      "RE Scores Computed:  GBR\n",
      "TTE Scores Computed:  GBR\n",
      "Grammar Scores Computed:  GBR\n",
      "Processing Country:  FRA\n",
      "Waves Detected:  FRA\n",
      "Trends Detected:  FRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:140: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Generation Complete:  FRA\n",
      "Top-k Complete:  FRA\n",
      "Top-p Complete:  FRA\n",
      "RE Scores Computed:  FRA\n",
      "TTE Scores Computed:  FRA\n",
      "Grammar Scores Computed:  FRA\n",
      "Processing Country:  ESP\n",
      "Waves Detected:  ESP\n",
      "Trends Detected:  ESP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:140: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Generation Complete:  ESP\n",
      "Top-k Complete:  ESP\n",
      "Top-p Complete:  ESP\n",
      "RE Scores Computed:  ESP\n",
      "TTE Scores Computed:  ESP\n",
      "Grammar Scores Computed:  ESP\n",
      "Processing Country:  ITA\n",
      "Waves Detected:  ITA\n",
      "Trends Detected:  ITA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:140: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Generation Complete:  ITA\n",
      "Top-k Complete:  ITA\n",
      "Top-p Complete:  ITA\n",
      "RE Scores Computed:  ITA\n",
      "TTE Scores Computed:  ITA\n",
      "Grammar Scores Computed:  ITA\n",
      "Processing Country:  TUR\n",
      "Waves Detected:  TUR\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3cbb279d8891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m#Detecting Trends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"RUS\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0msegmentation_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry_pop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0msegmentation_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msliding_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry_pop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Segmentation/segmentation.py\u001b[0m in \u001b[0;36mswab\u001b[0;34m(data, buffer_percent, bottom_up_error, best_line_error)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0msegments_retrieved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbottomupsegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msumsquared_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom_up_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegments_retrieved\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_retrieved\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mreturn_segments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_retrieved\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_retrieved\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#RE Scores\n",
    "template_re_scores = []\n",
    "t5_re_scores = []\n",
    "t5_re_scores_topk = []\n",
    "t5_re_scores_topp = []\n",
    "bart_re_scores = []\n",
    "bart_re_scores_topk = []\n",
    "bart_re_scores_topp = []\n",
    "\n",
    "#Diveristy Scores\n",
    "template_tte_scores = []\n",
    "t5_tte_scores = []\n",
    "t5_tte_scores_topk = []\n",
    "t5_tte_scores_topp = []\n",
    "bart_tte_scores = []\n",
    "bart_tte_scores_topk = []\n",
    "bart_tte_scores_topp = []\n",
    "\n",
    "#Grammar Scores\n",
    "t5_g_scores = []\n",
    "t5_g_scores_topk = []\n",
    "t5_g_scores_topp = []\n",
    "bart_g_scores = []\n",
    "bart_g_scores_topk = []\n",
    "bart_g_scores_topp = []\n",
    "\n",
    "#Grammar Mistakes\n",
    "t5_g_mistake = []\n",
    "t5_g_mistake_topk = []\n",
    "t5_g_mistake_topp = []\n",
    "bart_g_mistake = []\n",
    "bart_g_mistake_topk = []\n",
    "bart_g_mistake_topp = []\n",
    "\n",
    "countries = ['USA', 'IND', 'BRA', 'RUS', 'GBR', 'FRA', 'ESP', 'ITA' , 'TUR',  'DEU']\n",
    "\n",
    "for c in countries:\n",
    "    \n",
    "    print(\"Processing Country: \", c)\n",
    "    \n",
    "    country = ds_pop[ds_pop['Code']==c][['Population by Country (Clio Infra (2016))','Year']].reset_index().drop(columns=['index'])\n",
    "    country_pop_raw = country['Population by Country (Clio Infra (2016))'].tolist()\n",
    "\n",
    "    #Log-normalize data\n",
    "    trans = np.ma.log(country_pop_raw)\n",
    "    country_pop = trans.filled(0)\n",
    "    \n",
    "    #Detecting Waves\n",
    "    embeds, cluster_labels = segmentation.tslr_rep(country_pop)\n",
    "    cluster_arrangement = utils.find_contiguous(cluster_labels)\n",
    "    indices = utils.find_indices(cluster_arrangement)\n",
    "    wave_indices = utils.find_waves(country_pop_raw, indices, tolerance=7)\n",
    "    \n",
    "    print(\"Waves Detected: \", c)\n",
    "\n",
    "    #Detecting Trends\n",
    "    if c != \"RUS\":\n",
    "        segmentation_results = segmentation.swab(country_pop, 0.1, 3 ,3)\n",
    "    else:\n",
    "        segmentation_results = segmentation.sliding_window(country_pop, 1.5)\n",
    "    filtered_results = segmentation.re_segment(segmentation_results, country_pop)\n",
    "    trends = segmentation.find_trend(filtered_results, country_pop)\n",
    "    \n",
    "    print(\"Trends Detected: \", c)\n",
    "    \n",
    "    location = ds_pop[ds_pop['Code']==c]['Entity'].iloc[0]\n",
    "    \n",
    "    graph, essentials = data2graph.build_graph_pop_form1(\"Population data\", location, wave_indices, trends, country, country_pop_raw )\n",
    "    \n",
    "    #Template Narrative\n",
    "    template_text = data2graph.build_template_pop_nums(\"Population data\", location, wave_indices, trends, country, country_pop_raw )\n",
    "    \n",
    "    t5_prefix = 'translate Graph to English: '\n",
    "    \n",
    "    iso = c\n",
    "    \n",
    "    #Simple PLM Generation\n",
    "    t5_narrative = graph2text_nobeam(t5, graph, t5_prefix, 512, cuda3)\n",
    "    bart_narrative = graph2text_nobeam(bart , graph, \"\", 512, cuda3)\n",
    "    bart_narrative = re.sub('</s>' , '', bart_narrative)\n",
    "    \n",
    "    print(\"Simple Generation Complete: \", iso)\n",
    "    \n",
    "    #Top-k at 50\n",
    "    t5_narrative_topk = graph2text_nobeam_topk(t5, graph, t5_prefix, 50, 512, cuda3)\n",
    "    bart_narrative_topk = graph2text_nobeam_topk(bart, graph, \"\", 50, 512, cuda3)\n",
    "    bart_narrative_topk = re.sub('</s>' , '', bart_narrative_topk)\n",
    "    \n",
    "    print(\"Top-k Complete: \", iso)\n",
    "    \n",
    "    #Top-p at 0.92\n",
    "    t5_narrative_topp = graph2text_nobeam_topp(t5, graph, t5_prefix, 0.92, 512, cuda3)\n",
    "    bart_narrative_topp = graph2text_nobeam_topp(bart, graph, \"\", 0.92, 512, cuda3)\n",
    "    bart_narrative_topp = re.sub('</s>' , '', bart_narrative_topp)\n",
    "    \n",
    "    print(\"Top-p Complete: \", iso)\n",
    "    \n",
    "    #RE Scores\n",
    "    template_re_scores.append(textstat.flesch_reading_ease(template_text))\n",
    "    t5_re_scores.append(textstat.flesch_reading_ease(t5_narrative))\n",
    "    t5_re_scores_topk.append(textstat.flesch_reading_ease(t5_narrative_topk))\n",
    "    t5_re_scores_topp.append(textstat.flesch_reading_ease(t5_narrative_topp))\n",
    "    bart_re_scores.append(textstat.flesch_reading_ease(bart_narrative))\n",
    "    bart_re_scores_topk.append(textstat.flesch_reading_ease(bart_narrative_topk))\n",
    "    bart_re_scores_topp.append(textstat.flesch_reading_ease(bart_narrative_topp))\n",
    "    \n",
    "    print(\"RE Scores Computed: \", iso)\n",
    "    \n",
    "    #Diveristy Scores\n",
    "    template_tte_scores.append(ld.ttr(ld.flemmatize(template_text)))\n",
    "    t5_tte_scores.append(ld.ttr(ld.flemmatize(t5_narrative)))\n",
    "    t5_tte_scores_topk.append(ld.ttr(ld.flemmatize(t5_narrative_topk)))\n",
    "    t5_tte_scores_topp.append(ld.ttr(ld.flemmatize(t5_narrative_topp)))\n",
    "    bart_tte_scores.append(ld.ttr(ld.flemmatize(bart_narrative)))\n",
    "    bart_tte_scores_topk.append(ld.ttr(ld.flemmatize(bart_narrative_topk)))\n",
    "    bart_tte_scores_topp.append(ld.ttr(ld.flemmatize(bart_narrative_topp)))\n",
    "    \n",
    "    print(\"TTE Scores Computed: \", iso)\n",
    "    \n",
    "    #Grammar Scores\n",
    "    gs = grammar_score(t5_narrative)\n",
    "    t5_g_scores.append(gs)\n",
    "    if gs != 1.0:\n",
    "        t5_g_mistake.append((graph, t5_narrative))\n",
    "    \n",
    "    gs = grammar_score(t5_narrative_topk)\n",
    "    t5_g_scores_topk.append(gs)\n",
    "    if gs != 1.0:\n",
    "        t5_g_mistake_topk.append((graph, t5_narrative_topk))\n",
    "    \n",
    "    gs = grammar_score(t5_narrative_topp)\n",
    "    t5_g_scores_topp.append(gs)\n",
    "    if gs != 1.0:\n",
    "        t5_g_mistake_topp.append((graph, t5_narrative_topp))\n",
    "    \n",
    "    gs = grammar_score(bart_narrative)                          \n",
    "    bart_g_scores.append(gs)\n",
    "    if gs != 1.0:\n",
    "        bart_g_mistake.append((graph, bart_narrative))\n",
    "        \n",
    "    gs = grammar_score(bart_narrative_topk)\n",
    "    bart_g_scores_topk.append(gs)\n",
    "    if gs != 1.0:\n",
    "        bart_g_mistake_topk.append((graph, bart_narrative_topk))\n",
    "    \n",
    "    gs = grammar_score(bart_narrative_topp)\n",
    "    bart_g_scores_topp.append(gs)\n",
    "    if gs != 1.0:\n",
    "        bart_g_mistake_topp.append((graph, bart_narrative_topp))\n",
    "    \n",
    "    print(\"Grammar Scores Computed: \", iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** RE Scores ***\n",
      "template_re_scores:  66.28\n",
      "t5_re_scores:  69.57624999999999\n",
      "t5_re_scores_topk:  74.19375000000001\n",
      "t5_re_scores_topp:  71.82624999999999\n",
      "bart_re_scores:  75.04875\n",
      "bart_re_scores_topk:  76.8175\n",
      "bart_re_scores_topp:  76.58375000000001\n",
      "\n",
      "\n",
      "*** Diversity Scores ***\n",
      "template_tte_scores:  0.46777487189137357\n",
      "t5_tte_scores:  0.49512116482339175\n",
      "t5_tte_scores_topk:  0.5599216064094512\n",
      "t5_tte_scores_topp:  0.5365553504968045\n",
      "bart_tte_scores:  0.5486699256103651\n",
      "bart_tte_scores_topk:  0.5572979177683555\n",
      "bart_tte_scores_topp:  0.5698693064182194\n",
      "\n",
      "\n",
      "*** Grammar Scores ***\n",
      "t5_g_scores:  1.0\n",
      "t5_g_scores_topk:  1.0\n",
      "t5_g_scores_topp:  1.0\n",
      "bart_g_scores:  0.998046875\n",
      "bart_g_scores_topk:  0.9865077741407529\n",
      "bart_g_scores_topp:  0.9955021902377972\n"
     ]
    }
   ],
   "source": [
    "#RE Scores\n",
    "print(\"*** RE Scores ***\")\n",
    "print(\"template_re_scores: \", np.mean(template_re_scores))\n",
    "print(\"t5_re_scores: \", np.mean(t5_re_scores))\n",
    "print(\"t5_re_scores_topk: \", np.mean(t5_re_scores_topk))\n",
    "print(\"t5_re_scores_topp: \", np.mean(t5_re_scores_topp))\n",
    "print(\"bart_re_scores: \", np.mean(bart_re_scores))\n",
    "print(\"bart_re_scores_topk: \", np.mean(bart_re_scores_topk))\n",
    "print(\"bart_re_scores_topp: \", np.mean(bart_re_scores_topp))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"*** Diversity Scores ***\")\n",
    "#Diveristy Scores\n",
    "print(\"template_tte_scores: \", np.mean(template_tte_scores))\n",
    "print(\"t5_tte_scores: \", np.mean(t5_tte_scores))\n",
    "print(\"t5_tte_scores_topk: \", np.mean(t5_tte_scores_topk))\n",
    "print(\"t5_tte_scores_topp: \", np.mean(t5_tte_scores_topp))\n",
    "print(\"bart_tte_scores: \", np.mean(bart_tte_scores))\n",
    "print(\"bart_tte_scores_topk: \", np.mean(bart_tte_scores_topk))\n",
    "print(\"bart_tte_scores_topp: \", np.mean(bart_tte_scores_topp))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"*** Grammar Scores ***\")\n",
    "#Grammar Scores\n",
    "print(\"t5_g_scores: \", np.mean(t5_g_scores))\n",
    "print(\"t5_g_scores_topk: \", np.mean(t5_g_scores_topk))\n",
    "print(\"t5_g_scores_topp: \", np.mean(t5_g_scores_topp))\n",
    "print(\"bart_g_scores: \", np.mean(bart_g_scores))\n",
    "print(\"bart_g_scores_topk: \", np.mean(bart_g_scores_topk))\n",
    "print(\"bart_g_scores_topp: \", np.mean(bart_g_scores_topp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
