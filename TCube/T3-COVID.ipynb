{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import math\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation\n",
    "import utils\n",
    "import data2graph\n",
    "from finetuned import T5FineTuner, BARTFineTuner, generate, generate_beam, graph2text_nobeam, graph2text_nobeam_ngram_es, graph2text_nobeam_topk, graph2text_nobeam_topp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "import language_tool_python\n",
    "from lexical_diversity import lex_div as ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "def grammar_score(input_text):\n",
    "    errors = len(tool.check(input_text))\n",
    "    clean_text = input_text.translate(str.maketrans('', '', string.punctuation))\n",
    "    clean_text = list(filter(None, clean_text.split(' ')))\n",
    "    num_words = len(clean_text)\n",
    "    return float(1-(errors/num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Fine-Tuned PLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BARTFineTuner(\n",
       "  (model): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): Embedding(50268, 1024)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): Embedding(50268, 1024)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): Embedding(50268, 1024)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# cuda0 = torch.device(\"cuda:0\")\n",
    "# cuda1 = torch.device(\"cuda:1\")\n",
    "cuda3 = torch.device(\"cuda:3\")\n",
    "\n",
    "t5 = T5FineTuner.load_from_checkpoint(\"T5Models/T5Both.ckpt\")\n",
    "bart = BARTFineTuner.load_from_checkpoint(\"BARTModels/BARTBoth.ckpt\")\n",
    "\n",
    "t5.to(cuda3)\n",
    "bart.to(cuda3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVID19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Covid Dataset\n",
    "\n",
    "ds_covid = pd.read_csv(\"Data/COVID/owid-covid-data.csv\")\n",
    "#If missing values: Interpolate: ds_covid = ds_covid.interpolate(method='nearest')\n",
    "ds_covid = ds_covid.fillna(0)\n",
    "ds_covid.date = pd.to_datetime(ds_covid.date)\n",
    "ds_covid['month'] = pd.DatetimeIndex(ds_covid['date']).month\n",
    "ds_covid['month'] = ds_covid['month'].apply(lambda x: calendar.month_name[x])\n",
    "ds_covid['year'] = pd.DatetimeIndex(ds_covid['date']).year\n",
    "ds_covid.set_index(['date'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Country:  USA\n",
      "Waves Detected:  USA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  USA\n",
      "Simple Generation Complete:  USA\n",
      "Top-k Complete:  USA\n",
      "Top-p Complete:  USA\n",
      "RE Scores Computed:  USA\n",
      "TTE Scores Computed:  USA\n",
      "Grammar Scores Computed:  USA\n",
      "Processing Country:  IND\n",
      "Waves Detected:  IND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:140: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  IND\n",
      "Simple Generation Complete:  IND\n",
      "Top-k Complete:  IND\n",
      "Top-p Complete:  IND\n",
      "RE Scores Computed:  IND\n",
      "TTE Scores Computed:  IND\n",
      "Grammar Scores Computed:  IND\n",
      "Processing Country:  BRA\n",
      "Waves Detected:  BRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:140: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  BRA\n",
      "Simple Generation Complete:  BRA\n",
      "Top-k Complete:  BRA\n",
      "Top-p Complete:  BRA\n",
      "RE Scores Computed:  BRA\n",
      "TTE Scores Computed:  BRA\n",
      "Grammar Scores Computed:  BRA\n",
      "Processing Country:  RUS\n",
      "Waves Detected:  RUS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:140: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  RUS\n",
      "Simple Generation Complete:  RUS\n",
      "Top-k Complete:  RUS\n",
      "Top-p Complete:  RUS\n",
      "RE Scores Computed:  RUS\n",
      "TTE Scores Computed:  RUS\n",
      "Grammar Scores Computed:  RUS\n",
      "Processing Country:  GBR\n",
      "Waves Detected:  GBR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:140: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  GBR\n",
      "Simple Generation Complete:  GBR\n",
      "Top-k Complete:  GBR\n",
      "Top-p Complete:  GBR\n",
      "RE Scores Computed:  GBR\n",
      "TTE Scores Computed:  GBR\n",
      "Grammar Scores Computed:  GBR\n",
      "Processing Country:  FRA\n",
      "Waves Detected:  FRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:140: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  FRA\n",
      "Simple Generation Complete:  FRA\n",
      "Top-k Complete:  FRA\n",
      "Top-p Complete:  FRA\n",
      "RE Scores Computed:  FRA\n",
      "TTE Scores Computed:  FRA\n",
      "Grammar Scores Computed:  FRA\n",
      "Processing Country:  ESP\n",
      "Waves Detected:  ESP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:140: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  ESP\n",
      "Simple Generation Complete:  ESP\n",
      "Top-k Complete:  ESP\n",
      "Top-p Complete:  ESP\n",
      "RE Scores Computed:  ESP\n",
      "TTE Scores Computed:  ESP\n",
      "Grammar Scores Computed:  ESP\n",
      "Processing Country:  ITA\n",
      "Waves Detected:  ITA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:140: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  ITA\n",
      "Simple Generation Complete:  ITA\n",
      "Top-k Complete:  ITA\n",
      "Top-p Complete:  ITA\n",
      "RE Scores Computed:  ITA\n",
      "TTE Scores Computed:  ITA\n",
      "Grammar Scores Computed:  ITA\n",
      "Processing Country:  TUR\n",
      "Waves Detected:  TUR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:140: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  TUR\n",
      "Simple Generation Complete:  TUR\n",
      "Top-k Complete:  TUR\n",
      "Top-p Complete:  TUR\n",
      "RE Scores Computed:  TUR\n",
      "TTE Scores Computed:  TUR\n",
      "Grammar Scores Computed:  TUR\n",
      "Processing Country:  DEU\n",
      "Waves Detected:  DEU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandarsharma/Segmentation/segmentation.py:28: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  (p,residuals,rank,s) = np.linalg.lstsq(A,y)\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:140: RuntimeWarning: invalid value encountered in sqrt\n",
      "  t = r * np.sqrt(df / ((1.0 - r + TINY)*(1.0 + r + TINY)))\n",
      "/home/mandarsharma/trans/lib/python3.6/site-packages/scipy/stats/_stats_mstats_common.py:142: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends Detected:  DEU\n",
      "Simple Generation Complete:  DEU\n",
      "Top-k Complete:  DEU\n",
      "Top-p Complete:  DEU\n",
      "RE Scores Computed:  DEU\n",
      "TTE Scores Computed:  DEU\n",
      "Grammar Scores Computed:  DEU\n"
     ]
    }
   ],
   "source": [
    "countries = ['USA', 'IND', 'BRA', 'RUS', 'GBR', 'FRA', 'ESP', 'ITA' , 'TUR',  'DEU']\n",
    "\n",
    "#RE Scores\n",
    "template_re_scores = []\n",
    "t5_re_scores = []\n",
    "t5_re_scores_topk = []\n",
    "t5_re_scores_topp = []\n",
    "bart_re_scores = []\n",
    "bart_re_scores_topk = []\n",
    "bart_re_scores_topp = []\n",
    "\n",
    "#Diveristy Scores\n",
    "template_tte_scores = []\n",
    "t5_tte_scores = []\n",
    "t5_tte_scores_topk = []\n",
    "t5_tte_scores_topp = []\n",
    "bart_tte_scores = []\n",
    "bart_tte_scores_topk = []\n",
    "bart_tte_scores_topp = []\n",
    "\n",
    "#Grammar Scores\n",
    "t5_g_scores = []\n",
    "t5_g_scores_topk = []\n",
    "t5_g_scores_topp = []\n",
    "bart_g_scores = []\n",
    "bart_g_scores_topk = []\n",
    "bart_g_scores_topp = []\n",
    "\n",
    "#Grammar Mistakes\n",
    "t5_g_mistake = []\n",
    "t5_g_mistake_topk = []\n",
    "t5_g_mistake_topp = []\n",
    "bart_g_mistake = []\n",
    "bart_g_mistake_topk = []\n",
    "bart_g_mistake_topp = []\n",
    "\n",
    "for iso in countries:\n",
    "    \n",
    "    print(\"Processing Country: \", iso)\n",
    "    \n",
    "    country = ds_covid[ds_covid['iso_code']==iso][['new_cases','month', 'year']].reset_index().drop(columns=['date'])\n",
    "    country_cases_raw = country['new_cases'].tolist()\n",
    "\n",
    "    #Log-normalize data\n",
    "    trans = np.ma.log(country_cases_raw)\n",
    "    country_cases = trans.filled(0)\n",
    "    \n",
    "    #Detecting Waves\n",
    "    embeds, cluster_labels = segmentation.tslr_rep(country_cases)\n",
    "    cluster_arrangement = utils.find_contiguous(cluster_labels)\n",
    "    indices = utils.find_indices(cluster_arrangement)\n",
    "    wave_indices = utils.find_waves(country_cases_raw, indices, tolerance=7)\n",
    "    \n",
    "    print(\"Waves Detected: \", iso)\n",
    "\n",
    "    #Detecting Trends\n",
    "    segmentation_results = segmentation.swab(country_cases, 0.1, 3, 3)\n",
    "    filtered_results = segmentation.re_segment(segmentation_results, country_cases)\n",
    "    trends = segmentation.find_trend(filtered_results, country_cases)\n",
    "    \n",
    "    print(\"Trends Detected: \", iso)\n",
    "    \n",
    "    location = ds_covid[ds_covid['iso_code'] == iso].iloc[1]['location']\n",
    "    \n",
    "    graph, essentials = data2graph.build_graph_covid_form1(\"Coronavirus cases\", location, wave_indices, trends, country, country_cases_raw )\n",
    "    \n",
    "    #Template Narrative\n",
    "    template_text = data2graph.build_template_covid_nums(\"Coronavirus cases\", location, wave_indices, trends, country, country_cases_raw )\n",
    "\n",
    "    t5_prefix = 'translate Graph to English: '\n",
    "    \n",
    "    #Simple PLM Generation\n",
    "    t5_narrative = graph2text_nobeam(t5, graph, t5_prefix, 512, cuda3)\n",
    "    bart_narrative = graph2text_nobeam(bart , graph, \"\", 512, cuda3)\n",
    "    bart_narrative = re.sub('</s>' , '', bart_narrative)\n",
    "    \n",
    "    print(\"Simple Generation Complete: \", iso)\n",
    "    \n",
    "    #Top-k at 50\n",
    "    t5_narrative_topk = graph2text_nobeam_topk(t5, graph, t5_prefix, 50, 512, cuda3)\n",
    "    bart_narrative_topk = graph2text_nobeam_topk(bart, graph, \"\", 50, 512, cuda3)\n",
    "    bart_narrative_topk = re.sub('</s>' , '', bart_narrative_topk)\n",
    "    \n",
    "    print(\"Top-k Complete: \", iso)\n",
    "    \n",
    "    #Top-p at 0.92\n",
    "    t5_narrative_topp = graph2text_nobeam_topp(t5, graph, t5_prefix, 0.92, 512, cuda3)\n",
    "    bart_narrative_topp = graph2text_nobeam_topp(bart, graph, \"\", 0.92, 512, cuda3)\n",
    "    bart_narrative_topp = re.sub('</s>' , '', bart_narrative_topp)\n",
    "    \n",
    "    print(\"Top-p Complete: \", iso)\n",
    "    \n",
    "    #RE Scores\n",
    "    template_re_scores.append(textstat.flesch_reading_ease(template_text))\n",
    "    t5_re_scores.append(textstat.flesch_reading_ease(t5_narrative))\n",
    "    t5_re_scores_topk.append(textstat.flesch_reading_ease(t5_narrative_topk))\n",
    "    t5_re_scores_topp.append(textstat.flesch_reading_ease(t5_narrative_topp))\n",
    "    bart_re_scores.append(textstat.flesch_reading_ease(bart_narrative))\n",
    "    bart_re_scores_topk.append(textstat.flesch_reading_ease(bart_narrative_topk))\n",
    "    bart_re_scores_topp.append(textstat.flesch_reading_ease(bart_narrative_topp))\n",
    "    \n",
    "    print(\"RE Scores Computed: \", iso)\n",
    "    \n",
    "    #Diveristy Scores\n",
    "    template_tte_scores.append(ld.ttr(ld.flemmatize(template_text)))\n",
    "    t5_tte_scores.append(ld.ttr(ld.flemmatize(t5_narrative)))\n",
    "    t5_tte_scores_topk.append(ld.ttr(ld.flemmatize(t5_narrative_topk)))\n",
    "    t5_tte_scores_topp.append(ld.ttr(ld.flemmatize(t5_narrative_topp)))\n",
    "    bart_tte_scores.append(ld.ttr(ld.flemmatize(bart_narrative)))\n",
    "    bart_tte_scores_topk.append(ld.ttr(ld.flemmatize(bart_narrative_topk)))\n",
    "    bart_tte_scores_topp.append(ld.ttr(ld.flemmatize(bart_narrative_topp)))\n",
    "    \n",
    "    print(\"TTE Scores Computed: \", iso)\n",
    "    \n",
    "    #Grammar Scores\n",
    "    gs = grammar_score(t5_narrative)\n",
    "    t5_g_scores.append(gs)\n",
    "    if gs != 1.0:\n",
    "        t5_g_mistake.append((graph, t5_narrative))\n",
    "    \n",
    "    gs = grammar_score(t5_narrative_topk)\n",
    "    t5_g_scores_topk.append(gs)\n",
    "    if gs != 1.0:\n",
    "        t5_g_mistake_topk.append((graph, t5_narrative_topk))\n",
    "    \n",
    "    gs = grammar_score(t5_narrative_topp)\n",
    "    t5_g_scores_topp.append(gs)\n",
    "    if gs != 1.0:\n",
    "        t5_g_mistake_topp.append((graph, t5_narrative_topp))\n",
    "    \n",
    "    gs = grammar_score(bart_narrative)                          \n",
    "    bart_g_scores.append(gs)\n",
    "    if gs != 1.0:\n",
    "        bart_g_mistake.append((graph, bart_narrative))\n",
    "        \n",
    "    gs = grammar_score(bart_narrative_topk)\n",
    "    bart_g_scores_topk.append(gs)\n",
    "    if gs != 1.0:\n",
    "        bart_g_mistake_topk.append((graph, bart_narrative_topk))\n",
    "    \n",
    "    gs = grammar_score(bart_narrative_topp)\n",
    "    bart_g_scores_topp.append(gs)\n",
    "    if gs != 1.0:\n",
    "        bart_g_mistake_topp.append((graph, bart_narrative_topp))\n",
    "    \n",
    "    print(\"Grammar Scores Computed: \", iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** RE Scores ***\n",
      "template_re_scores:  17.794000000000004\n",
      "t5_re_scores:  64.486\n",
      "t5_re_scores_topk:  65.678\n",
      "t5_re_scores_topp:  68.024\n",
      "bart_re_scores:  70.71000000000001\n",
      "bart_re_scores_topk:  69.608\n",
      "bart_re_scores_topp:  67.532\n",
      "\n",
      "\n",
      "*** Diversity Scores ***\n",
      "template_tte_scores:  0.2662341503911566\n",
      "t5_tte_scores:  0.3109472873333881\n",
      "t5_tte_scores_topk:  0.38714269563265336\n",
      "t5_tte_scores_topp:  0.3699915339415927\n",
      "bart_tte_scores:  0.4250835914750163\n",
      "bart_tte_scores_topk:  0.4294212038876563\n",
      "bart_tte_scores_topp:  0.4099059446725753\n",
      "\n",
      "\n",
      "*** Grammar Scores ***\n",
      "t5_g_scores:  0.99434795361903\n",
      "t5_g_scores_topk:  0.9899279793236557\n",
      "t5_g_scores_topp:  0.9928866408072878\n",
      "bart_g_scores:  0.9388890914596416\n",
      "bart_g_scores_topk:  0.9398873855847419\n",
      "bart_g_scores_topp:  0.9430885059553639\n"
     ]
    }
   ],
   "source": [
    "#RE Scores\n",
    "print(\"*** RE Scores ***\")\n",
    "print(\"template_re_scores: \", np.mean(template_re_scores))\n",
    "print(\"t5_re_scores: \", np.mean(t5_re_scores))\n",
    "print(\"t5_re_scores_topk: \", np.mean(t5_re_scores_topk))\n",
    "print(\"t5_re_scores_topp: \", np.mean(t5_re_scores_topp))\n",
    "print(\"bart_re_scores: \", np.mean(bart_re_scores))\n",
    "print(\"bart_re_scores_topk: \", np.mean(bart_re_scores_topk))\n",
    "print(\"bart_re_scores_topp: \", np.mean(bart_re_scores_topp))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"*** Diversity Scores ***\")\n",
    "#Diveristy Scores\n",
    "print(\"template_tte_scores: \", np.mean(template_tte_scores))\n",
    "print(\"t5_tte_scores: \", np.mean(t5_tte_scores))\n",
    "print(\"t5_tte_scores_topk: \", np.mean(t5_tte_scores_topk))\n",
    "print(\"t5_tte_scores_topp: \", np.mean(t5_tte_scores_topp))\n",
    "print(\"bart_tte_scores: \", np.mean(bart_tte_scores))\n",
    "print(\"bart_tte_scores_topk: \", np.mean(bart_tte_scores_topk))\n",
    "print(\"bart_tte_scores_topp: \", np.mean(bart_tte_scores_topp))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"*** Grammar Scores ***\")\n",
    "#Grammar Scores\n",
    "print(\"t5_g_scores: \", np.mean(t5_g_scores))\n",
    "print(\"t5_g_scores_topk: \", np.mean(t5_g_scores_topk))\n",
    "print(\"t5_g_scores_topp: \", np.mean(t5_g_scores_topp))\n",
    "print(\"bart_g_scores: \", np.mean(bart_g_scores))\n",
    "print(\"bart_g_scores_topk: \", np.mean(bart_g_scores_topk))\n",
    "print(\"bart_g_scores_topp: \", np.mean(bart_g_scores_topp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
